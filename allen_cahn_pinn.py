# -*- coding: utf-8 -*-
"""Allen-Cahn_PINN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M3EIjcujO67B5g6iFwdlaWlwDY8yAwMU
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import random
from torch import Tensor, optim
import numpy as np
from numpy import zeros, sum, sqrt, linspace, absolute, sin, cos, pi
from matplotlib.pyplot import plot, legend, imshow, show, clf, pause, semilogy, title, xlabel, ylabel, subplot, xlim, ylim, figure, savefig
import matplotlib.pyplot as plt
import sys
import time
################## cuda #################
def print_gpu_memory():
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.memory_allocated(i) / 1024 ** 2:.2f} MiB used out of {torch.cuda.get_device_properties(i).total_memory / 1024 ** 2:.2f} MiB")

# Example usage
print_gpu_memory()
# Check if CUDA is available and set the default tensor type accordingly
print("torch.cuda.device_count() = ",torch.cuda.device_count())
print("torch.cuda.get_device_name(0) = ",torch.cuda.get_device_name(0))
device = torch.device("cuda:1") if torch.cuda.is_available() else torch.device("cpu")
if torch.cuda.is_available():
    print("CUDA is available. Setting default tensor type to CUDA tensor.")
    torch.set_default_tensor_type('torch.cuda.FloatTensor')
else:
    print("CUDA is not available. Using CPU.")
    torch.set_default_tensor_type('torch.FloatTensor')
################## cuda #################
################## Parameters #################
torch.set_default_dtype(torch.float64)
n_iteration = 20000
num_show = 100
space_dim = 4
input_dim = space_dim + 1
m = 200
N_domain = 5000
penalty_boun = 1000
sigma = 1
activation_type = 'tanh'
################## Parameters #################
################## cuda #################
def numerical_gradient_wrt_time(x, function):
  h = 0.01
  gradient_u_t = torch.zeros((x.shape[0],1))
  e_j = torch.zeros_like(x)
  e_j[:,-1] = 1
  direction_gradient = (function(x+e_j*h) - function(x-e_j*h))/h/2
  gradient_u_t = direction_gradient
  del e_j, direction_gradient
  torch.cuda.empty_cache()
  return gradient_u_t
def numerical_gradient(x, function, skeme):
  h = 0.01
  u_x = function(x)
  if x.dim() == 1 and u_x.dim() == 1:
    if skeme == "forward":
      return (function(x+h) - function(x))/h
    if skeme == "central":
      return (function(x+h) - function(x-h))/h/2
  if x.dim() == 2 and u_x.dim() == 1:
    print("Error message: x.dim() == 2 and u_x.dim() == 1")
  if x.dim() == 1 and u_x.dim() == 2:
    print("Error message: x.dim() == 1 and u_x.dim() == 2")
  if x.dim() == 2 and u_x.dim() == 2:
    gradient_u_x = torch.zeros_like(x)
    for dim in range(x.shape[1]):
      e_j = torch.zeros_like(x)
      e_j[:,dim] = 1
      if skeme == "forward":
        direction_gradient = (function(x+e_j*h) - function(x))/h
      if skeme == "central":
        direction_gradient = (function(x+e_j*h) - function(x-e_j*h))/h/2
      gradient_u_x[:,dim] = direction_gradient.squeeze()
    del u_x, e_j, direction_gradient
    torch.cuda.empty_cache()
    return gradient_u_x


def numerical_hessian(x, function):
  # for case 4: input x should be nxdim, function output nx1
  h = 0.01
  u_x = function(x)
  if x.dim() == 1 and u_x.dim() == 1:
    return (function(x+h) + function(x-h) - 2*function(x))/h/h
  if x.dim() == 2 and u_x.dim() == 1:
    print("Error message: x.dim() == 2 and u_x.dim() == 1")
  if x.dim() == 1 and u_x.dim() == 2:
    print("Error message: x.dim() == 1 and u_x.dim() == 2")
  if x.dim() == 2 and u_x.dim() == 2:
    hessian_u_x = torch.zeros((x.shape[0],x.shape[1],x.shape[1]))
    for i in range(x.shape[1]):
      gradient_u_x = torch.zeros_like(x)
      for dim in range(x.shape[1]):
        e_j = torch.zeros_like(x)
        e_j[:,dim] = 1
        direction_gradient = (numerical_gradient(x+e_j*h, function, "central")[:,i] - numerical_gradient(x-e_j*h, function, "central")[:,i])/h/2
        gradient_u_x[:,dim] = direction_gradient.squeeze()
      hessian_u_x[:,i,:] = gradient_u_x
    return hessian_u_x

def numerical_divergence_of_composition(x, function, model):
  # input x should be in shape nxdim, function should output nxdim
  # note that funciton(x, model), function must have two inputs: x and model
  h = 0.01
  u_x = function(x, model)
  if x.dim() == 1 and u_x.dim() == 1:
    return (function(x+h, model)+function(x-h, model)-function(x, model))/h/h
  if x.dim() == 2 and u_x.dim() == 1:
    print("Error message: x.dim() == 2 and u_x.dim() == 1")
  if x.dim() == 1 and u_x.dim() == 2:
    print("Error message: x.dim() == 1 and u_x.dim() == 2")
  if x.dim() == 2 and u_x.dim() == 2:
    divergence = torch.zeros_like(function(x, model)[:,0].unsqueeze(1))
    for i in range(x.shape[1]):
      e_j = torch.zeros_like(x)
      e_j[:,i] = 1
      f_0 = function(x + e_j*h, model)[:,i].unsqueeze(1)
      f_1 = function(x - e_j*h, model)[:,i].unsqueeze(1)
      partial_F_i_partial_x_i = (f_0 - f_1)/h/2
      divergence += partial_F_i_partial_x_i
    del u_x, e_j, partial_F_i_partial_x_i, f_0, f_1
    torch.cuda.empty_cache()
    return divergence

def numerical_divergence(x, function):
  # for case 4: x: nxdim, function(x): nxdim
  h = 0.01
  u_x = function(x)
  if x.dim() == 1 and u_x.dim() == 1:
    return (function(x+h)+function(x-h)-function(x))/h/h
  if x.dim() == 2 and u_x.dim() == 1:
    print("Error message: x.dim() == 2 and u_x.dim() == 1")
  if x.dim() == 1 and u_x.dim() == 2:
    print("Error message: x.dim() == 1 and u_x.dim() == 2")
  if x.dim() == 2 and u_x.dim() == 2:
    divergence = torch.zeros_like(function(x)[:,0].unsqueeze(1))
    for i in range(x.shape[1]):
      e_j = torch.zeros_like(x)
      e_j[:,i] = 1
      partial_F_i_partial_x_i = (function(x + e_j*h)[:,i].unsqueeze(1) - function(x - e_j*h)[:,i].unsqueeze(1))/h/2
      divergence += partial_F_i_partial_x_i
      del u_x, e_j, partial_F_i_partial_x_i
      torch.cuda.empty_cache()
    return divergence
################## Problem setting #################
def gradient_u(x,model):
  x = numerical_gradient(x,model,"central")
  return x
def h(input):
  x = input[:, :-1]
  norm_x = torch.sqrt(torch.sum(x*x, dim = 1))
  out = torch.sin(np.pi*0.5*(torch.abs(1-norm_x))**2.5).unsqueeze(1)
  del x, norm_x
  torch.cuda.empty_cache()
  return out
def l(input):
  x = input[:, :-1]
  norm_x = torch.sqrt(torch.sum(x*x, dim = 1))
  out = torch.cos(np.pi*0.5*(torch.abs(1-norm_x))**2.5).unsqueeze(1)
  del x, norm_x
  torch.cuda.empty_cache()
  return out
def k(input):
  x = input[:, :-1]
  norm_x = torch.sqrt(torch.sum(x*x, dim = 1)).unsqueeze(1)
  temp = torch.abs(1-norm_x)
  out = -5/4*np.pi*(space_dim-1)*norm_x*l(input)*temp**1.5 - 25/16*np.pi**2*h(input)*(1-norm_x)**3 + 15/8*np.pi*l(input)*temp**0.5
  del x, norm_x, temp
  torch.cuda.empty_cache()
  return out
def u_exact(input):
  x = input[:, :-1]
  t = input[:,-1].unsqueeze(1)
  norm_x = torch.sqrt(torch.sum(x*x, dim = 1)).unsqueeze(1)
  out = torch.exp(-t)*torch.sin(np.pi*0.5*(torch.abs(1-norm_x))**2.5)
  del x, t, norm_x
  torch.cuda.empty_cache()
  return out
def f_tensor(input):
  t = input[:,-1].unsqueeze(1)
  out = - torch.exp(-t)*(h(input) + k(input)) - u_exact(input) + u_exact(input)**3
  del t
  torch.cuda.empty_cache()
  return out
def g(x):
  return torch.zeros((x.shape[0],1))
################## Problem setting #################
################## Generate Data #################
def data_boun(n_points, n_dim):
  n = n_dim  # Dimension
  num_points = n_points  # Number of points to generate
  # Generate points from a normal distribution
  points = np.random.randn(num_points, n)
  # Normalize each point to lie on the surface of the unit hypersphere
  normalized_points = points / np.linalg.norm(points, axis=1, keepdims=True)
  t = torch.rand((num_points,1))
  x = torch.tensor(normalized_points)
  data = torch.hstack((x,t))
  return data
def data_domain(N_domain, dim):
  n = dim  # Dimension of each point
  n_data = N_domain
  valid_points = []  # List to store valid points
  while len(valid_points) < n_data:
      # Generate a single point
      point = np.random.uniform(-1, 1, n)
      # Check if the point is inside the hypersphere
      if np.linalg.norm(point) < 1:
          valid_points.append(point)
  # Convert the list of valid points to a numpy array
  valid_points = np.array(valid_points)
  x = torch.tensor(valid_points)
  t = torch.rand((n_data,1))
  data = torch.hstack((x,t))
  del x, t
  torch.cuda.empty_cache()
  return data
def data_init(N_domain, dim):
  n = dim  # Dimension of each point
  n_data = N_domain
  valid_points = []  # List to store valid points
  while len(valid_points) < n_data:
      # Generate a single point
      point = np.random.uniform(-1, 1, n)
      # Check if the point is inside the hypersphere
      if np.linalg.norm(point) < 1:
          valid_points.append(point)
  # Convert the list of valid points to a numpy array
  valid_points = np.array(valid_points)
  x = torch.tensor(valid_points)
  t = torch.zeros((n_data,1))
  data = torch.hstack((x,t))
  return data
def data_visualise():
  num_points = 100
  x = torch.ones((num_points, space_dim+1))
  t_range = torch.tensor(np.linspace(0, 1, num_points))
  x[:,-1] = t_range
  coefficient = torch.zeros((space_dim+1))
  coefficient[0] = 0.5
  coefficient[-1] = 1
  x = x*coefficient
  return x, t_range
def visualise(model):
  x_plot, t_range = data_visualise()
  t_range = t_range.cpu().detach().numpy()
  y_model = model(x_plot).cpu().detach().numpy()
  y_exact = u_exact(x_plot).cpu().detach().numpy()
  figure()
  plt.plot(t_range, y_model,'b', t_range, y_exact, 'r')
  plt.xlim([0,1])
  plt.xlabel('t')
  plt.ylabel('value')
  plt.title('Visualization')
  plt.show()
################## Generate Data #################
class Network(nn.Module):
    def __init__(self):
      super(Network, self).__init__()

      if activation_type == 'tanh' :
        activation = nn.Tanh()
      if activation_type == 'relu' :
        activation = nn.ReLU()

      self.activation = activation
      self.linear1 = torch.nn.Linear(input_dim, m)
      self.linear2 = torch.nn.Linear(m, m)
      self.linear3 = torch.nn.Linear(m, m)
      self.linear4 = torch.nn.Linear(m, m)
      self.linear5 = torch.nn.Linear(m, m)
      self.linear6 = torch.nn.Linear(m, m)
      self.linear_out = torch.nn.Linear(m, 1)

    # x represents our data
    def forward(self, input):
        x = self.activation(self.linear1(input))
        x = x + self.activation(self.linear2(x))
        x = x + self.activation(self.linear3(x))
        x = x + self.activation(self.linear4(x))
        x = x + self.activation(self.linear5(x))
        x = x + self.activation(self.linear6(x))
        x = self.linear_out(x)
        return x




def loss_domain_calcul(x_tensor, f_tensor, model):
  f_x_tensor = f_tensor(x_tensor) # f_tensor = 0)
  u = model(x_tensor)
  laplace_u = numerical_divergence_of_composition(x_tensor, gradient_u, model)
  u_t = numerical_gradient_wrt_time(x_tensor, model)
  N_u = u_t - laplace_u - u + u**3
  loss_domain = (N_u - f_x_tensor)**2
  del x_tensor, laplace_u
  torch.cuda.empty_cache()
  return loss_domain


def train():
  model = Network()
  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
  x_error = data_domain(N_domain,space_dim)
  for i in range(n_iteration):
    x_domain = data_domain(N_domain,space_dim)
    x_boun = data_boun(N_domain,space_dim)
    x_init = data_init(N_domain,space_dim)

    loss = torch.mean(loss_domain_calcul(x_domain, f_tensor, model))
    loss = loss + penalty_boun*torch.mean((model(x_init) - h(x_init))**2) + penalty_boun*torch.mean((model(x_boun) - g(x_boun))**2)
    error = torch.mean((u_exact(x_error) - model(x_error))**2)



    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (i+1)%(n_iteration/num_show) == 0:
      print()
      print(f'Epoch: {i//int(n_iteration/num_show)+1} '
        f'loss: {loss:.5f}'
        f' error: {error:.5f}'
        )
      visualise(model)


train()







# n_dim = 5
# N_domain = 1000
# def data_boun(n_points, n_dim):
#   n = n_dim  # Dimension
#   num_points = n_points  # Number of points to generate
#   # Generate points from a normal distribution
#   points = np.random.randn(num_points, n)
#   # Normalize each point to lie on the surface of the unit hypersphere
#   normalized_points = points / np.linalg.norm(points, axis=1, keepdims=True)
#   t = torch.rand((num_points,1))
#   x = torch.tensor(normalized_points)
#   data = torch.hstack((x,t))
#   return data